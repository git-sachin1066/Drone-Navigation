{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.legacy.nn import Reshape\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "import time\n",
    "from math import log,sqrt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def InitializeWeights(mod):\n",
    "    for m in mod.modules():\n",
    "        if isinstance(m,nn.Conv2d):\n",
    "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            #print m.weight.size(), m.out_channels, m.in_channels\n",
    "            m.weight.data.normal_(0,sqrt(2./n))\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1)\n",
    "            m.bias.data.zero_()\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corridor (\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(7, 7), stride=(2, 2))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (conv3): Conv2d(16, 20, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear (640 -> 120)\n",
      "  (fc2): Linear (120 -> 84)\n",
      "  (fc3): Linear (84 -> 10)\n",
      "  (fc4): Linear (10 -> 1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Corridor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Corridor, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = InitializeWeights(nn.Conv2d(3, 6, 7, 2))\n",
    "        self.conv2 = InitializeWeights(nn.Conv2d(6, 16, 5, 2))\n",
    "        self.conv3 = InitializeWeights(nn.Conv2d(16, 20, 3, 1))\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(20 * 4 * 8, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.fc4 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Corridor()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "torch.Size([6, 3, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers --->  14\n",
      "Total number of parameters --->  94149\n"
     ]
    }
   ],
   "source": [
    "sum1 = 0\n",
    "        \n",
    "print(\"Number of layers ---> \",len(list(net.parameters())))\n",
    "for params in net.parameters():\n",
    "    if params.requires_grad == True:\n",
    "        sum1 += params.numel()\n",
    "    \n",
    "print(\"Total number of parameters ---> \",sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.1117\n",
      " 0.1128\n",
      " 0.1382\n",
      " 0.1100\n",
      " 0.1073\n",
      "[torch.FloatTensor of size 5x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.randn(5, 3, 180, 320))\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = h5py.File('./DATASET/CODE/NewTrainData.h5')\n",
    "xtrainT = torch.from_numpy(np.array(file['xtrain'],dtype=np.float32)).float()\n",
    "ytrainT = torch.from_numpy(np.array(file['ytrain'],dtype=np.float32)).float()\n",
    "#xtrain = np.array(file['xtrain'],dtype=np.float32)\n",
    "#ytrain = np.array(file['ytrain'],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = h5py.File('./DATASET/CODE/NewTestData.h5')\n",
    "xtestT = torch.from_numpy(np.array(file['xtest'],dtype=np.float32)).float()\n",
    "ytestT = torch.from_numpy(np.array(file['ytest'],dtype=np.float32)).float()\n",
    "#xtest = np.array(file['xtest'],dtype=np.float32)\n",
    "#ytest = np.array(file['ytest'],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_rgb_to_bgr(batch):\n",
    "    #print(batch.size())\n",
    "    (r, g, b) = torch.chunk(batch, 3, 1)\n",
    "    #print(r.size())\n",
    "    batch1 = torch.cat((b, g, r),1)\n",
    "    #print(batch1.size())\n",
    "    return batch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11884, 3, 180, 320]) torch.Size([2516, 3, 180, 320])\n"
     ]
    }
   ],
   "source": [
    "xtrainT = batch_rgb_to_bgr(xtrainT)\n",
    "xtestT = batch_rgb_to_bgr(xtestT)\n",
    "print(xtrainT.size(), xtestT.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrainT = torch.div(xtrainT,255.0)\n",
    "xtestT = torch.div(xtestT,255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.min(xtrainT), torch.max(xtrainT), torch.min(xtestT), torch.max(xtestT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11884, 3, 180, 320]) torch.Size([11884, 2]) torch.Size([2516, 3, 180, 320]) torch.Size([2516, 2])\n"
     ]
    }
   ],
   "source": [
    "print(xtrainT.size(), ytrainT.size(), xtestT.size(), ytestT.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    \"\"\"\n",
    "    Normalize an tensor image with mean and standard deviation.\n",
    "    Given mean: (R, G, B) and std: (R, G, B),\n",
    "    will normalize each channel of the torch.*Tensor, i.e.\n",
    "    channel = (channel - mean) / std\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for R, G, B channels respecitvely.\n",
    "        std (sequence): Sequence of standard deviations for R, G, B channels\n",
    "            respecitvely.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        # TODO: make efficient\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.sub_(m).div_(s)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mn = [0.406,0.456,0.485]\n",
    "sd = [0.225,0.224,0.229]\n",
    "norm = Normalize(mn,sd)\n",
    "xtrainT = norm(xtrainT)\n",
    "xtestT = norm(xtestT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0836544036865234 2.640000104904175 -2.1179039478302 2.640000104904175\n"
     ]
    }
   ],
   "source": [
    "print(torch.min(xtrainT), torch.max(xtrainT), torch.min(xtestT), torch.max(xtestT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##def train(model, loss, optimizer, x_val, y_val, validPixel, batch_sz):\n",
    "def train(model, loss, optimizer, x_val, y_val):\n",
    "    x = Variable(x_val,requires_grad = False).cuda()\n",
    "    y = Variable(y_val,requires_grad = False).cuda()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    fx = model.forward(x)\n",
    "    \n",
    "    #print fx.data[0][0][64][87]\n",
    "    #fx = model5.forward(Variable(xtest2[start:end], volatile=True).cuda())\n",
    "    ##output = loss.forward(fx,y,validPixel,batch_sz)\n",
    "    output = loss.forward(fx,y)\n",
    "    #output = loss(fx, y)\n",
    "    output.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return output.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#custom loss function.... this will be reverse Huber...\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        \n",
    "    def forward(self,inp, tar):\n",
    "        #target is the ground truth value...\n",
    "        #k = torch.mean(inp[:,0])\n",
    "        '''\n",
    "        if (k >= 1.48 and k <= 1.65):\n",
    "            diff = torch.abs(tar[:,1]-inp[:,1])\n",
    "            loss = torch.mean(torch.pow(diff,2))\n",
    "        else:\n",
    "        '''\n",
    "        diff = torch.abs(tar[:,0]-inp[:,0]) #*(180/np.pi)\n",
    "        loss = torch.mean(torch.pow(diff,2))\n",
    "        #print(loss)\n",
    "        return loss\n",
    "        '''\n",
    "        c1 = c.data[0] \n",
    "        temp = diff > c1\n",
    "        check1 = torch.prod(temp)\n",
    "        \n",
    "        if check1 == 0:\n",
    "            lossval = torch.mean(diff)\n",
    "        else:\n",
    "            temp4 = torch.pow(diff,2)\n",
    "            d = torch.pow(c,2)\n",
    "            temp4 = temp4.add(d.expand_as(temp4))\n",
    "            lossval = torch.mean(temp4/(2*c))\n",
    "        return lossval\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save the weights\n",
      "Loss = 0.6642 at epoch 1 completed in 0m 8s\n",
      "save the weights\n",
      "Loss = 0.3396 at epoch 2 completed in 0m 7s\n",
      "save the weights\n",
      "Loss = 0.3360 at epoch 3 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.3309 at epoch 4 completed in 0m 7s\n",
      "save the weights\n",
      "Loss = 0.3250 at epoch 5 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.3168 at epoch 6 completed in 0m 7s\n",
      "save the weights\n",
      "Loss = 0.3038 at epoch 7 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.2822 at epoch 8 completed in 0m 7s\n",
      "save the weights\n",
      "Loss = 0.2514 at epoch 9 completed in 0m 7s\n",
      "save the weights\n",
      "Loss = 0.2234 at epoch 10 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.2026 at epoch 11 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.1873 at epoch 12 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.1717 at epoch 13 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.1568 at epoch 14 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.1423 at epoch 15 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.1274 at epoch 16 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.1143 at epoch 17 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.1044 at epoch 18 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0923 at epoch 19 completed in 0m 7s\n",
      "save the weights\n",
      "Loss = 0.0853 at epoch 20 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0796 at epoch 21 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0739 at epoch 22 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0705 at epoch 23 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0635 at epoch 24 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0602 at epoch 25 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0580 at epoch 26 completed in 0m 7s\n",
      "save the weights\n",
      "Loss = 0.0569 at epoch 27 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0520 at epoch 28 completed in 0m 7s\n",
      "save the weights\n",
      "Loss = 0.0501 at epoch 29 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0472 at epoch 30 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0466 at epoch 31 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0439 at epoch 32 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0416 at epoch 33 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0403 at epoch 34 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0384 at epoch 35 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0366 at epoch 36 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0352 at epoch 37 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0340 at epoch 38 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0331 at epoch 39 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0320 at epoch 40 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0305 at epoch 41 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0296 at epoch 42 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0284 at epoch 43 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0281 at epoch 44 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0274 at epoch 45 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0265 at epoch 46 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0256 at epoch 47 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0244 at epoch 48 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0242 at epoch 49 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0231 at epoch 50 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0224 at epoch 51 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0223 at epoch 52 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0212 at epoch 53 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0208 at epoch 54 completed in 0m 6s\n",
      "Loss 0.020876999434886447 is bigger than Loss 0.020848525829161656 in the prev epoch \n",
      "Loss = 0.0209 at epoch 55 completed in 0m 7s\n",
      "save the weights\n",
      "Loss = 0.0198 at epoch 56 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0195 at epoch 57 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0187 at epoch 58 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0180 at epoch 59 completed in 0m 6s\n",
      "Loss 0.01809448787632088 is bigger than Loss 0.01800832385902828 in the prev epoch \n",
      "Loss = 0.0181 at epoch 60 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0176 at epoch 61 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0175 at epoch 62 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0167 at epoch 63 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0166 at epoch 64 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0161 at epoch 65 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0157 at epoch 66 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0154 at epoch 67 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0150 at epoch 68 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0144 at epoch 69 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0143 at epoch 70 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0140 at epoch 71 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0137 at epoch 72 completed in 0m 6s\n",
      "Loss 0.01375760896655069 is bigger than Loss 0.013680870079918316 in the prev epoch \n",
      "Loss = 0.0138 at epoch 73 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0130 at epoch 74 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0128 at epoch 75 completed in 0m 7s\n",
      "save the weights\n",
      "Loss = 0.0127 at epoch 76 completed in 0m 6s\n",
      "Loss 0.01297938751342145 is bigger than Loss 0.012690417971054366 in the prev epoch \n",
      "Loss = 0.0130 at epoch 77 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0123 at epoch 78 completed in 0m 6s\n",
      "Loss 0.012296910574879206 is bigger than Loss 0.012285875027171437 in the prev epoch \n",
      "Loss = 0.0123 at epoch 79 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0120 at epoch 80 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0114 at epoch 81 completed in 0m 6s\n",
      "Loss 0.011448444384275641 is bigger than Loss 0.011423743690555347 in the prev epoch \n",
      "Loss = 0.0114 at epoch 82 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0111 at epoch 83 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0109 at epoch 84 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0108 at epoch 85 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0106 at epoch 86 completed in 0m 6s\n",
      "Loss 0.011086031371219593 is bigger than Loss 0.01057572037597761 in the prev epoch \n",
      "Loss = 0.0111 at epoch 87 completed in 0m 7s\n",
      "save the weights\n",
      "Loss = 0.0103 at epoch 88 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0103 at epoch 89 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0101 at epoch 90 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0097 at epoch 91 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0096 at epoch 92 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0095 at epoch 93 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0092 at epoch 94 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0091 at epoch 95 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0090 at epoch 96 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0088 at epoch 97 completed in 0m 6s\n",
      "Loss 0.008818273159349166 is bigger than Loss 0.008809983101203058 in the prev epoch \n",
      "Loss = 0.0088 at epoch 98 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0087 at epoch 99 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0084 at epoch 100 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0083 at epoch 101 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0082 at epoch 102 completed in 0m 6s\n",
      "Loss 0.008322185726392617 is bigger than Loss 0.008248392215017412 in the prev epoch \n",
      "Loss = 0.0083 at epoch 103 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0079 at epoch 104 completed in 0m 6s\n",
      "Loss 0.008109698740973992 is bigger than Loss 0.007936073361695295 in the prev epoch \n",
      "Loss = 0.0081 at epoch 105 completed in 0m 6s\n",
      "Loss 0.008075307936506404 is bigger than Loss 0.007936073361695295 in the prev epoch \n",
      "Loss = 0.0081 at epoch 106 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0078 at epoch 107 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0076 at epoch 108 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0075 at epoch 109 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0075 at epoch 110 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0073 at epoch 111 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0071 at epoch 112 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0070 at epoch 113 completed in 0m 6s\n",
      "Loss 0.007071791492286528 is bigger than Loss 0.007002139447557956 in the prev epoch \n",
      "Loss = 0.0071 at epoch 114 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0068 at epoch 115 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0067 at epoch 116 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0067 at epoch 117 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0065 at epoch 118 completed in 0m 6s\n",
      "Loss 0.006550979192771633 is bigger than Loss 0.0065353085749048825 in the prev epoch \n",
      "Loss = 0.0066 at epoch 119 completed in 0m 6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save the weights\n",
      "Loss = 0.0065 at epoch 120 completed in 0m 6s\n",
      "Loss 0.006560462462497872 is bigger than Loss 0.006482192694588947 in the prev epoch \n",
      "Loss = 0.0066 at epoch 121 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0063 at epoch 122 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0060 at epoch 123 completed in 0m 6s\n",
      "Loss 0.00610612289978486 is bigger than Loss 0.006034344676388148 in the prev epoch \n",
      "Loss = 0.0061 at epoch 124 completed in 0m 6s\n",
      "Loss 0.0062380509743245865 is bigger than Loss 0.006034344676388148 in the prev epoch \n",
      "Loss = 0.0062 at epoch 125 completed in 0m 6s\n",
      "Loss 0.006055989050265614 is bigger than Loss 0.006034344676388148 in the prev epoch \n",
      "Loss = 0.0061 at epoch 126 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0059 at epoch 127 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0059 at epoch 128 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0057 at epoch 129 completed in 0m 6s\n",
      "Loss 0.0056941708601205305 is bigger than Loss 0.005666617537113308 in the prev epoch \n",
      "Loss = 0.0057 at epoch 130 completed in 0m 6s\n",
      "Loss 0.005708166238477905 is bigger than Loss 0.005666617537113308 in the prev epoch \n",
      "Loss = 0.0057 at epoch 131 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0056 at epoch 132 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0056 at epoch 133 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0055 at epoch 134 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0054 at epoch 135 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0053 at epoch 136 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0051 at epoch 137 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0051 at epoch 138 completed in 0m 6s\n",
      "Loss 0.005153874231475756 is bigger than Loss 0.0051232306572831275 in the prev epoch \n",
      "Loss = 0.0052 at epoch 139 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0051 at epoch 140 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0049 at epoch 141 completed in 0m 6s\n",
      "Loss 0.0050019319904168556 is bigger than Loss 0.004871039919442306 in the prev epoch \n",
      "Loss = 0.0050 at epoch 142 completed in 0m 6s\n",
      "Loss 0.004919246566616728 is bigger than Loss 0.004871039919442306 in the prev epoch \n",
      "Loss = 0.0049 at epoch 143 completed in 0m 6s\n",
      "Loss 0.004878005455992801 is bigger than Loss 0.004871039919442306 in the prev epoch \n",
      "Loss = 0.0049 at epoch 144 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0049 at epoch 145 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0048 at epoch 146 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0047 at epoch 147 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0047 at epoch 148 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0046 at epoch 149 completed in 0m 6s\n",
      "Loss 0.004618382357532843 is bigger than Loss 0.0045786709001410155 in the prev epoch \n",
      "Loss = 0.0046 at epoch 150 completed in 0m 6s\n",
      "Loss 0.0046328220266365905 is bigger than Loss 0.0045786709001410155 in the prev epoch \n",
      "Loss = 0.0046 at epoch 151 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0045 at epoch 152 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0045 at epoch 153 completed in 0m 7s\n",
      "save the weights\n",
      "Loss = 0.0044 at epoch 154 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0043 at epoch 155 completed in 0m 6s\n",
      "Loss 0.004397656946711132 is bigger than Loss 0.004294474662112242 in the prev epoch \n",
      "Loss = 0.0044 at epoch 156 completed in 0m 6s\n",
      "Loss 0.004307796287382498 is bigger than Loss 0.004294474662112242 in the prev epoch \n",
      "Loss = 0.0043 at epoch 157 completed in 0m 6s\n",
      "Loss 0.004401901225921593 is bigger than Loss 0.004294474662112242 in the prev epoch \n",
      "Loss = 0.0044 at epoch 158 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0042 at epoch 159 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0041 at epoch 160 completed in 0m 6s\n",
      "Loss 0.004267177621556466 is bigger than Loss 0.004133150612375685 in the prev epoch \n",
      "Loss = 0.0043 at epoch 161 completed in 0m 6s\n",
      "Loss 0.004296009379312837 is bigger than Loss 0.004133150612375685 in the prev epoch \n",
      "Loss = 0.0043 at epoch 162 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0040 at epoch 163 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0040 at epoch 164 completed in 0m 6s\n",
      "Loss 0.004022824410027717 is bigger than Loss 0.003983735561767527 in the prev epoch \n",
      "Loss = 0.0040 at epoch 165 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0039 at epoch 166 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0039 at epoch 167 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0039 at epoch 168 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0039 at epoch 169 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0037 at epoch 170 completed in 0m 6s\n",
      "Loss 0.0038526902473664934 is bigger than Loss 0.003747453869191886 in the prev epoch \n",
      "Loss = 0.0039 at epoch 171 completed in 0m 6s\n",
      "Loss 0.003800993326754287 is bigger than Loss 0.003747453869191886 in the prev epoch \n",
      "Loss = 0.0038 at epoch 172 completed in 0m 6s\n",
      "Loss 0.0037645776448410745 is bigger than Loss 0.003747453869191886 in the prev epoch \n",
      "Loss = 0.0038 at epoch 173 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0036 at epoch 174 completed in 0m 6s\n",
      "Loss 0.003678658279638913 is bigger than Loss 0.003628961251078876 in the prev epoch \n",
      "Loss = 0.0037 at epoch 175 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0036 at epoch 176 completed in 0m 6s\n",
      "Loss 0.003678013184025526 is bigger than Loss 0.003578179896207877 in the prev epoch \n",
      "Loss = 0.0037 at epoch 177 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0036 at epoch 178 completed in 0m 6s\n",
      "Loss 0.003574565184590335 is bigger than Loss 0.003558284361507299 in the prev epoch \n",
      "Loss = 0.0036 at epoch 179 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0035 at epoch 180 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0034 at epoch 181 completed in 0m 6s\n",
      "Loss 0.0034521582765898247 is bigger than Loss 0.0034432295716315914 in the prev epoch \n",
      "Loss = 0.0035 at epoch 182 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0034 at epoch 183 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0034 at epoch 184 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0033 at epoch 185 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0033 at epoch 186 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0032 at epoch 187 completed in 0m 6s\n",
      "Loss 0.003289776469987695 is bigger than Loss 0.0032082879259282423 in the prev epoch \n",
      "Loss = 0.0033 at epoch 188 completed in 0m 7s\n",
      "Loss 0.003256223915612814 is bigger than Loss 0.0032082879259282423 in the prev epoch \n",
      "Loss = 0.0033 at epoch 189 completed in 0m 6s\n",
      "Loss 0.0032695749533905184 is bigger than Loss 0.0032082879259282423 in the prev epoch \n",
      "Loss = 0.0033 at epoch 190 completed in 0m 6s\n",
      "Loss 0.0032871471525300045 is bigger than Loss 0.0032082879259282423 in the prev epoch \n",
      "Loss = 0.0033 at epoch 191 completed in 0m 6s\n",
      "Learning rate changed from 0.0001 to 2e-05\n",
      "Loss 0.003227648147995668 is bigger than Loss 0.0032082879259282423 in the prev epoch \n",
      "Loss = 0.0032 at epoch 192 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0029 at epoch 193 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0028 at epoch 194 completed in 0m 6s\n",
      "Loss 0.0028380372928790993 is bigger than Loss 0.0028281738474590376 in the prev epoch \n",
      "Loss = 0.0028 at epoch 195 completed in 0m 6s\n",
      "Loss 0.00283148304068543 is bigger than Loss 0.0028281738474590376 in the prev epoch \n",
      "Loss = 0.0028 at epoch 196 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0028 at epoch 197 completed in 0m 6s\n",
      "Loss 0.0028360278916157947 is bigger than Loss 0.002821894041348802 in the prev epoch \n",
      "Loss = 0.0028 at epoch 198 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0028 at epoch 199 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0028 at epoch 200 completed in 0m 6s\n",
      "Loss 0.002824810624392297 is bigger than Loss 0.002804098912159623 in the prev epoch \n",
      "Loss = 0.0028 at epoch 201 completed in 0m 6s\n",
      "Loss 0.0028062661038084514 is bigger than Loss 0.002804098912159623 in the prev epoch \n",
      "Loss = 0.0028 at epoch 202 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0028 at epoch 203 completed in 0m 6s\n",
      "Loss 0.0028007124277828915 is bigger than Loss 0.00279544001088089 in the prev epoch \n",
      "Loss = 0.0028 at epoch 204 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0028 at epoch 205 completed in 0m 6s\n",
      "Loss 0.0028005487849796355 is bigger than Loss 0.0027855365965923015 in the prev epoch \n",
      "Loss = 0.0028 at epoch 206 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0028 at epoch 207 completed in 0m 6s\n",
      "Loss 0.002771540234486264 is bigger than Loss 0.002766989680968997 in the prev epoch \n",
      "Loss = 0.0028 at epoch 208 completed in 0m 6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save the weights\n",
      "Loss = 0.0028 at epoch 209 completed in 0m 6s\n",
      "Loss 0.0027773937179866926 is bigger than Loss 0.0027553974612583077 in the prev epoch \n",
      "Loss = 0.0028 at epoch 210 completed in 0m 6s\n",
      "Loss 0.002764571821458187 is bigger than Loss 0.0027553974612583077 in the prev epoch \n",
      "Loss = 0.0028 at epoch 211 completed in 0m 6s\n",
      "Loss 0.002760422187904367 is bigger than Loss 0.0027553974612583077 in the prev epoch \n",
      "Loss = 0.0028 at epoch 212 completed in 0m 6s\n",
      "Loss 0.0027613749309961908 is bigger than Loss 0.0027553974612583077 in the prev epoch \n",
      "Loss = 0.0028 at epoch 213 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0028 at epoch 214 completed in 0m 6s\n",
      "Loss 0.0027625354139960553 is bigger than Loss 0.0027553698862118866 in the prev epoch \n",
      "Loss = 0.0028 at epoch 215 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0027 at epoch 216 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0027 at epoch 217 completed in 0m 6s\n",
      "Loss 0.0027269547617436093 is bigger than Loss 0.0027261337884980094 in the prev epoch \n",
      "Loss = 0.0027 at epoch 218 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0027 at epoch 219 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0027 at epoch 220 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0027 at epoch 221 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0027 at epoch 222 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0027 at epoch 223 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0027 at epoch 224 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0027 at epoch 225 completed in 0m 6s\n",
      "Loss 0.002699465071664879 is bigger than Loss 0.0026911663300920622 in the prev epoch \n",
      "Loss = 0.0027 at epoch 226 completed in 0m 6s\n",
      "Loss 0.002700391861097418 is bigger than Loss 0.0026911663300920622 in the prev epoch \n",
      "Loss = 0.0027 at epoch 227 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0027 at epoch 228 completed in 0m 6s\n",
      "Loss 0.0026861545889969992 is bigger than Loss 0.002676655081930172 in the prev epoch \n",
      "Loss = 0.0027 at epoch 229 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0027 at epoch 230 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0027 at epoch 231 completed in 0m 6s\n",
      "Loss 0.002669731793460289 is bigger than Loss 0.0026636024686564337 in the prev epoch \n",
      "Loss = 0.0027 at epoch 232 completed in 0m 6s\n",
      "Loss 0.0026904891025759507 is bigger than Loss 0.0026636024686564337 in the prev epoch \n",
      "Loss = 0.0027 at epoch 233 completed in 0m 6s\n",
      "Loss 0.002684702683365215 is bigger than Loss 0.0026636024686564337 in the prev epoch \n",
      "Loss = 0.0027 at epoch 234 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0026 at epoch 235 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0026 at epoch 236 completed in 0m 6s\n",
      "Loss 0.0026639958454361835 is bigger than Loss 0.002642914301324602 in the prev epoch \n",
      "Loss = 0.0027 at epoch 237 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0026 at epoch 238 completed in 0m 6s\n",
      "Loss 0.0026425311743847784 is bigger than Loss 0.0026347142598561954 in the prev epoch \n",
      "Loss = 0.0026 at epoch 239 completed in 0m 6s\n",
      "Loss 0.0026552929505329536 is bigger than Loss 0.0026347142598561954 in the prev epoch \n",
      "Loss = 0.0027 at epoch 240 completed in 0m 6s\n",
      "Loss 0.0026444630623483102 is bigger than Loss 0.0026347142598561954 in the prev epoch \n",
      "Loss = 0.0026 at epoch 241 completed in 0m 6s\n",
      "Loss 0.0026459620293639036 is bigger than Loss 0.0026347142598561954 in the prev epoch \n",
      "Loss = 0.0026 at epoch 242 completed in 0m 6s\n",
      "Learning rate changed from 2e-05 to 4.000000000000001e-06\n",
      "Loss 0.0026372324160663925 is bigger than Loss 0.0026347142598561954 in the prev epoch \n",
      "Loss = 0.0026 at epoch 243 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0026 at epoch 244 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0026 at epoch 245 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0026 at epoch 246 completed in 0m 6s\n",
      "Loss 0.0025629635312498845 is bigger than Loss 0.0025619762217248135 in the prev epoch \n",
      "Loss = 0.0026 at epoch 247 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0026 at epoch 248 completed in 0m 6s\n",
      "Loss 0.002562139315181018 is bigger than Loss 0.002560663026222892 in the prev epoch \n",
      "Loss = 0.0026 at epoch 249 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0026 at epoch 250 completed in 0m 6s\n",
      "Loss 0.002561919943246086 is bigger than Loss 0.002557503983577531 in the prev epoch \n",
      "Loss = 0.0026 at epoch 251 completed in 0m 6s\n",
      "Loss 0.002560872904233549 is bigger than Loss 0.002557503983577531 in the prev epoch \n",
      "Loss = 0.0026 at epoch 252 completed in 0m 6s\n",
      "Loss 0.0025592494945305263 is bigger than Loss 0.002557503983577531 in the prev epoch \n",
      "Loss = 0.0026 at epoch 253 completed in 0m 6s\n",
      "Loss 0.002559761852178853 is bigger than Loss 0.002557503983577531 in the prev epoch \n",
      "Loss = 0.0026 at epoch 254 completed in 0m 6s\n",
      "Learning rate changed from 4.000000000000001e-06 to 8.000000000000002e-07\n",
      "Loss 0.0025580874197373158 is bigger than Loss 0.002557503983577531 in the prev epoch \n",
      "Loss = 0.0026 at epoch 255 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0025 at epoch 256 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0025 at epoch 257 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0025 at epoch 258 completed in 0m 6s\n",
      "Loss 0.0025416695921852703 is bigger than Loss 0.0025371204354558805 in the prev epoch \n",
      "Loss = 0.0025 at epoch 259 completed in 0m 6s\n",
      "Loss 0.002543064609823765 is bigger than Loss 0.0025371204354558805 in the prev epoch \n",
      "Loss = 0.0025 at epoch 260 completed in 0m 6s\n",
      "Loss 0.002541088602217135 is bigger than Loss 0.0025371204354558805 in the prev epoch \n",
      "Loss = 0.0025 at epoch 261 completed in 0m 6s\n",
      "Loss 0.0025426398804771913 is bigger than Loss 0.0025371204354558805 in the prev epoch \n",
      "Loss = 0.0025 at epoch 262 completed in 0m 6s\n",
      "Learning rate changed from 8.000000000000002e-07 to 1.6000000000000003e-07\n",
      "Loss 0.002540634694565652 is bigger than Loss 0.0025371204354558805 in the prev epoch \n",
      "Loss = 0.0025 at epoch 263 completed in 0m 6s\n",
      "Loss 0.002538765611688322 is bigger than Loss 0.0025371204354558805 in the prev epoch \n",
      "Loss = 0.0025 at epoch 264 completed in 0m 6s\n",
      "Loss 0.0025384938495268137 is bigger than Loss 0.0025371204354558805 in the prev epoch \n",
      "Loss = 0.0025 at epoch 265 completed in 0m 6s\n",
      "Loss 0.002538408225861625 is bigger than Loss 0.0025371204354558805 in the prev epoch \n",
      "Loss = 0.0025 at epoch 266 completed in 0m 6s\n",
      "Loss 0.0025389609136775445 is bigger than Loss 0.0025371204354558805 in the prev epoch \n",
      "Loss = 0.0025 at epoch 267 completed in 0m 6s\n",
      "Learning rate changed from 1.6000000000000003e-07 to 3.200000000000001e-08\n",
      "Loss 0.0025389541726620526 is bigger than Loss 0.0025371204354558805 in the prev epoch \n",
      "Loss = 0.0025 at epoch 268 completed in 0m 6s\n",
      "Loss 0.0025380410521325682 is bigger than Loss 0.0025371204354558805 in the prev epoch \n",
      "Loss = 0.0025 at epoch 269 completed in 0m 6s\n",
      "Loss 0.0025385748874278496 is bigger than Loss 0.0025371204354558805 in the prev epoch \n",
      "Loss = 0.0025 at epoch 270 completed in 0m 6s\n",
      "Loss 0.0025377543610779364 is bigger than Loss 0.0025371204354558805 in the prev epoch \n",
      "Loss = 0.0025 at epoch 271 completed in 0m 6s\n",
      "Loss 0.0025385103129855497 is bigger than Loss 0.0025371204354558805 in the prev epoch \n",
      "Loss = 0.0025 at epoch 272 completed in 0m 6s\n",
      "Learning rate changed from 3.200000000000001e-08 to 6.400000000000002e-09\n",
      "Loss 0.0025387259150435044 is bigger than Loss 0.0025371204354558805 in the prev epoch \n",
      "Loss = 0.0025 at epoch 273 completed in 0m 6s\n",
      "Loss 0.0025376043866877657 is bigger than Loss 0.0025371204354558805 in the prev epoch \n",
      "Loss = 0.0025 at epoch 274 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0025 at epoch 275 completed in 0m 6s\n",
      "Loss 0.002538214752318102 is bigger than Loss 0.0025369801854688095 in the prev epoch \n",
      "Loss = 0.0025 at epoch 276 completed in 0m 6s\n",
      "Loss 0.0025379853016384854 is bigger than Loss 0.0025369801854688095 in the prev epoch \n",
      "Loss = 0.0025 at epoch 277 completed in 0m 6s\n",
      "Loss 0.002537906011633154 is bigger than Loss 0.0025369801854688095 in the prev epoch \n",
      "Loss = 0.0025 at epoch 278 completed in 0m 6s\n",
      "Loss 0.0025377383694540324 is bigger than Loss 0.0025369801854688095 in the prev epoch \n",
      "Loss = 0.0025 at epoch 279 completed in 0m 6s\n",
      "Learning rate changed from 6.400000000000002e-09 to 1.2800000000000003e-09\n",
      "Loss 0.00253800719389253 is bigger than Loss 0.0025369801854688095 in the prev epoch \n",
      "Loss = 0.0025 at epoch 280 completed in 0m 6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.002538347691364165 is bigger than Loss 0.0025369801854688095 in the prev epoch \n",
      "Loss = 0.0025 at epoch 281 completed in 0m 6s\n",
      "Loss 0.002537633479935012 is bigger than Loss 0.0025369801854688095 in the prev epoch \n",
      "Loss = 0.0025 at epoch 282 completed in 0m 6s\n",
      "Loss 0.0025373641013478228 is bigger than Loss 0.0025369801854688095 in the prev epoch \n",
      "Loss = 0.0025 at epoch 283 completed in 0m 6s\n",
      "Loss 0.002538474979955447 is bigger than Loss 0.0025369801854688095 in the prev epoch \n",
      "Loss = 0.0025 at epoch 284 completed in 0m 6s\n",
      "Learning rate changed from 1.2800000000000003e-09 to 2.5600000000000005e-10\n",
      "Loss 0.0025382057599204377 is bigger than Loss 0.0025369801854688095 in the prev epoch \n",
      "Loss = 0.0025 at epoch 285 completed in 0m 6s\n",
      "Loss 0.002538534872698742 is bigger than Loss 0.0025369801854688095 in the prev epoch \n",
      "Loss = 0.0025 at epoch 286 completed in 0m 6s\n",
      "Loss 0.0025381843862437355 is bigger than Loss 0.0025369801854688095 in the prev epoch \n",
      "Loss = 0.0025 at epoch 287 completed in 0m 6s\n",
      "Loss 0.0025381919986102717 is bigger than Loss 0.0025369801854688095 in the prev epoch \n",
      "Loss = 0.0025 at epoch 288 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0025 at epoch 289 completed in 0m 6s\n",
      "Loss 0.0025383437832195003 is bigger than Loss 0.0025364898420754288 in the prev epoch \n",
      "Loss = 0.0025 at epoch 290 completed in 0m 6s\n",
      "Loss 0.0025382416936457616 is bigger than Loss 0.0025364898420754288 in the prev epoch \n",
      "Loss = 0.0025 at epoch 291 completed in 0m 6s\n",
      "Loss 0.0025374358022108878 is bigger than Loss 0.0025364898420754288 in the prev epoch \n",
      "Loss = 0.0025 at epoch 292 completed in 0m 6s\n",
      "Loss 0.0025379174692330493 is bigger than Loss 0.0025364898420754288 in the prev epoch \n",
      "Loss = 0.0025 at epoch 293 completed in 0m 6s\n",
      "Learning rate changed from 2.5600000000000005e-10 to 5.120000000000001e-11\n",
      "Loss 0.0025381268447371656 is bigger than Loss 0.0025364898420754288 in the prev epoch \n",
      "Loss = 0.0025 at epoch 294 completed in 0m 6s\n",
      "Loss 0.002538375726290304 is bigger than Loss 0.0025364898420754288 in the prev epoch \n",
      "Loss = 0.0025 at epoch 295 completed in 0m 6s\n",
      "Loss 0.0025380256840361833 is bigger than Loss 0.0025364898420754288 in the prev epoch \n",
      "Loss = 0.0025 at epoch 296 completed in 0m 6s\n",
      "Loss 0.002538370393135863 is bigger than Loss 0.0025364898420754288 in the prev epoch \n",
      "Loss = 0.0025 at epoch 297 completed in 0m 6s\n",
      "Loss 0.0025382864311328077 is bigger than Loss 0.0025364898420754288 in the prev epoch \n",
      "Loss = 0.0025 at epoch 298 completed in 0m 6s\n",
      "Learning rate changed from 5.120000000000001e-11 to 1.0240000000000002e-11\n",
      "Loss 0.002538262026640037 is bigger than Loss 0.0025364898420754288 in the prev epoch \n",
      "Loss = 0.0025 at epoch 299 completed in 0m 6s\n",
      "Loss 0.002537860212787225 is bigger than Loss 0.0025364898420754288 in the prev epoch \n",
      "Loss = 0.0025 at epoch 300 completed in 0m 6s\n",
      "Loss 0.0025371888635589146 is bigger than Loss 0.0025364898420754288 in the prev epoch \n",
      "Loss = 0.0025 at epoch 301 completed in 0m 6s\n",
      "Loss 0.002537526029572895 is bigger than Loss 0.0025364898420754288 in the prev epoch \n",
      "Loss = 0.0025 at epoch 302 completed in 0m 6s\n",
      "Loss 0.002537964464667777 is bigger than Loss 0.0025364898420754288 in the prev epoch \n",
      "Loss = 0.0025 at epoch 303 completed in 0m 6s\n",
      "Learning rate changed from 1.0240000000000002e-11 to 2.048e-12\n",
      "Loss 0.0025384101522020258 is bigger than Loss 0.0025364898420754288 in the prev epoch \n",
      "Loss = 0.0025 at epoch 304 completed in 0m 6s\n",
      "Loss 0.0025376118081645335 is bigger than Loss 0.0025364898420754288 in the prev epoch \n",
      "Loss = 0.0025 at epoch 305 completed in 0m 6s\n",
      "Loss 0.0025384826053550112 is bigger than Loss 0.0025364898420754288 in the prev epoch \n",
      "Loss = 0.0025 at epoch 306 completed in 0m 6s\n",
      "Loss 0.002538367484712673 is bigger than Loss 0.0025364898420754288 in the prev epoch \n",
      "Loss = 0.0025 at epoch 307 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0025 at epoch 308 completed in 0m 6s\n",
      "Loss 0.002537635643613723 is bigger than Loss 0.0025352810261385418 in the prev epoch \n",
      "Loss = 0.0025 at epoch 309 completed in 0m 6s\n",
      "Loss 0.0025378786767668313 is bigger than Loss 0.0025352810261385418 in the prev epoch \n",
      "Loss = 0.0025 at epoch 310 completed in 0m 6s\n",
      "Loss 0.0025385382473711786 is bigger than Loss 0.0025352810261385418 in the prev epoch \n",
      "Loss = 0.0025 at epoch 311 completed in 0m 6s\n",
      "Loss 0.0025385541904886988 is bigger than Loss 0.0025352810261385418 in the prev epoch \n",
      "Loss = 0.0025 at epoch 312 completed in 0m 6s\n",
      "Learning rate changed from 2.048e-12 to 4.0960000000000004e-13\n",
      "Loss 0.002538254198395594 is bigger than Loss 0.0025352810261385418 in the prev epoch \n",
      "Loss = 0.0025 at epoch 313 completed in 0m 6s\n",
      "Loss 0.0025380735160345502 is bigger than Loss 0.0025352810261385418 in the prev epoch \n",
      "Loss = 0.0025 at epoch 314 completed in 0m 6s\n",
      "Loss 0.002538597661126184 is bigger than Loss 0.0025352810261385418 in the prev epoch \n",
      "Loss = 0.0025 at epoch 315 completed in 0m 6s\n",
      "Loss 0.002536676844573336 is bigger than Loss 0.0025352810261385418 in the prev epoch \n",
      "Loss = 0.0025 at epoch 316 completed in 0m 6s\n",
      "Loss 0.002538411533360066 is bigger than Loss 0.0025352810261385418 in the prev epoch \n",
      "Loss = 0.0025 at epoch 317 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0025 at epoch 318 completed in 0m 6s\n",
      "Loss 0.0025371331891071627 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 319 completed in 0m 6s\n",
      "Loss 0.0025384436580603337 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 320 completed in 0m 6s\n",
      "Loss 0.0025384794070694282 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 321 completed in 0m 6s\n",
      "Loss 0.0025381851801805537 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 322 completed in 0m 6s\n",
      "Learning rate changed from 4.0960000000000004e-13 to 8.192e-14\n",
      "Loss 0.0025385722372154103 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 323 completed in 0m 6s\n",
      "Loss 0.0025384499450756682 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 324 completed in 0m 6s\n",
      "Loss 0.0025375482763645082 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 325 completed in 0m 6s\n",
      "Loss 0.002538381242691086 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 326 completed in 0m 6s\n",
      "Loss 0.0025381673315950743 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 327 completed in 0m 6s\n",
      "Learning rate changed from 8.192e-14 to 1.6384e-14\n",
      "Loss 0.002538428854695849 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 328 completed in 0m 6s\n",
      "Loss 0.002538605764142148 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 329 completed in 0m 6s\n",
      "Loss 0.0025363918350733527 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 330 completed in 0m 6s\n",
      "Loss 0.0025383341982600093 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 331 completed in 0m 6s\n",
      "Loss 0.0025380017562778765 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 332 completed in 0m 6s\n",
      "Learning rate changed from 1.6384e-14 to 3.2768000000000003e-15\n",
      "Loss 0.0025378176158526123 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 333 completed in 0m 6s\n",
      "Loss 0.002538424153300315 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 334 completed in 0m 6s\n",
      "Loss 0.0025354710143758725 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 335 completed in 0m 6s\n",
      "Loss 0.0025379748905025846 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 336 completed in 0m 6s\n",
      "Loss 0.0025379755150100295 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 337 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 3.2768000000000003e-15 to 1e-15\n",
      "Loss 0.0025383843035909144 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 338 completed in 0m 6s\n",
      "Loss 0.0025385335339225454 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 339 completed in 0m 6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.0025375399626642527 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 340 completed in 0m 6s\n",
      "Loss 0.0025379134671133876 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 341 completed in 0m 6s\n",
      "Loss 0.0025379404130467047 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 342 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025371457937115515 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 343 completed in 0m 6s\n",
      "Loss 0.002538192697886147 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 344 completed in 0m 6s\n",
      "Loss 0.0025377105228698536 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 345 completed in 0m 6s\n",
      "Loss 0.0025382359259917115 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 346 completed in 0m 6s\n",
      "Loss 0.0025384253523389327 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 347 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025382458601971637 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 348 completed in 0m 6s\n",
      "Loss 0.0025384318536642906 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 349 completed in 0m 6s\n",
      "Loss 0.002537719106050022 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 350 completed in 0m 6s\n",
      "Loss 0.0025380973050352 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 351 completed in 0m 6s\n",
      "Loss 0.0025382596662899912 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 352 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025374586985960784 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 353 completed in 0m 6s\n",
      "Loss 0.00253596196600992 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 354 completed in 0m 6s\n",
      "Loss 0.0025378393035959015 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 355 completed in 0m 6s\n",
      "Loss 0.002538352034204428 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 356 completed in 0m 6s\n",
      "Loss 0.002538306470442976 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 357 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025384602949661355 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 358 completed in 0m 6s\n",
      "Loss 0.002537268835495411 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 359 completed in 0m 6s\n",
      "Loss 0.002538227585637439 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 360 completed in 0m 6s\n",
      "Loss 0.002537819282316386 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 361 completed in 0m 6s\n",
      "Loss 0.0025375612450117603 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 362 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025384912143072555 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 363 completed in 0m 6s\n",
      "Loss 0.002537857655569855 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 364 completed in 0m 6s\n",
      "Loss 0.0025379881494531956 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 365 completed in 0m 6s\n",
      "Loss 0.002538446006553268 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 366 completed in 0m 6s\n",
      "Loss 0.002537133795681945 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 367 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.00253695485847281 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 368 completed in 0m 6s\n",
      "Loss 0.0025383254144887954 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 369 completed in 0m 6s\n",
      "Loss 0.002538260812118574 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 370 completed in 0m 6s\n",
      "Loss 0.0025381559285771126 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 371 completed in 0m 6s\n",
      "Loss 0.0025384299181125804 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 372 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.002538419338331246 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 373 completed in 0m 6s\n",
      "Loss 0.00253774726327104 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 374 completed in 0m 6s\n",
      "Loss 0.0025379203328735798 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 375 completed in 0m 6s\n",
      "Loss 0.0025385975129612296 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 376 completed in 0m 6s\n",
      "Loss 0.002538188623937838 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 377 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025379593183854064 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 378 completed in 0m 6s\n",
      "Loss 0.0025380052054247617 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 379 completed in 0m 6s\n",
      "Loss 0.00253745178795523 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 380 completed in 0m 6s\n",
      "Loss 0.002538286725110892 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 381 completed in 0m 6s\n",
      "Loss 0.002537588185065515 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 382 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025373676271249965 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 383 completed in 0m 6s\n",
      "Loss 0.002538326009402444 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 384 completed in 0m 6s\n",
      "Loss 0.0025382325523971936 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 385 completed in 0m 6s\n",
      "Loss 0.0025385378571642684 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 386 completed in 0m 6s\n",
      "Loss 0.002538287434676 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 387 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025363507943607306 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 388 completed in 0m 6s\n",
      "Loss 0.002536676100710789 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 389 completed in 0m 6s\n",
      "Loss 0.0025373390450076447 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 390 completed in 0m 6s\n",
      "Loss 0.0025373232731813614 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 391 completed in 0m 6s\n",
      "Loss 0.00253758048862125 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 392 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025381358036213244 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 393 completed in 0m 6s\n",
      "Loss 0.002538326036644415 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 394 completed in 0m 6s\n",
      "Loss 0.0025378709762068703 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 395 completed in 0m 6s\n",
      "Loss 0.002537672537373478 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 396 completed in 0m 6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.0025371573737043226 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 397 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025380873431958047 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 398 completed in 0m 7s\n",
      "Loss 0.0025381244915895795 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 399 completed in 0m 6s\n",
      "Loss 0.0025379702493725544 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 400 completed in 0m 6s\n",
      "Loss 0.002538289333578444 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 401 completed in 0m 6s\n",
      "Loss 0.002538143009514161 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 402 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025381606396739243 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 403 completed in 0m 6s\n",
      "Loss 0.002537858204328947 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 404 completed in 0m 6s\n",
      "Loss 0.0025378521229022853 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 405 completed in 0m 6s\n",
      "Loss 0.0025375315867386373 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 406 completed in 0m 6s\n",
      "Loss 0.00253801838191452 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 407 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025367939459420264 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 408 completed in 0m 6s\n",
      "Loss 0.0025376859695261786 is bigger than Loss 0.002534406456817253 in the prev epoch \n",
      "Loss = 0.0025 at epoch 409 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0025 at epoch 410 completed in 0m 6s\n",
      "Loss 0.002537793190683389 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 411 completed in 0m 6s\n",
      "Loss 0.0025383225235572986 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 412 completed in 0m 6s\n",
      "Loss 0.0025383293025449598 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 413 completed in 0m 6s\n",
      "Loss 0.0025383861944089556 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 414 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025382181858841424 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 415 completed in 0m 6s\n",
      "Loss 0.002537762393441162 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 416 completed in 0m 6s\n",
      "Loss 0.0025378986037773712 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 417 completed in 0m 6s\n",
      "Loss 0.0025384158240192214 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 418 completed in 0m 6s\n",
      "Loss 0.0025379814859989224 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 419 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025379131467752657 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 420 completed in 0m 6s\n",
      "Loss 0.002536546627764343 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 421 completed in 0m 6s\n",
      "Loss 0.0025383895714822147 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 422 completed in 0m 6s\n",
      "Loss 0.0025382746793588405 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 423 completed in 0m 6s\n",
      "Loss 0.0025384521764673304 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 424 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.002536774972400404 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 425 completed in 0m 6s\n",
      "Loss 0.0025384862137400256 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 426 completed in 0m 6s\n",
      "Loss 0.0025377329733882793 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 427 completed in 0m 6s\n",
      "Loss 0.0025380416531707653 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 428 completed in 0m 6s\n",
      "Loss 0.0025381507496631747 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 429 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025382599066660705 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 430 completed in 0m 6s\n",
      "Loss 0.0025384208056738657 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 431 completed in 0m 6s\n",
      "Loss 0.0025384360994917857 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 432 completed in 0m 6s\n",
      "Loss 0.002538577734997575 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 433 completed in 0m 6s\n",
      "Loss 0.002537311231349018 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 434 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025384591020030673 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 435 completed in 0m 6s\n",
      "Loss 0.0025379360302254243 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 436 completed in 0m 7s\n",
      "Loss 0.0025382850811854376 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 437 completed in 0m 7s\n",
      "Loss 0.0025372441567210885 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 438 completed in 0m 7s\n",
      "Loss 0.0025375346848756854 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 439 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.002538350173911102 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 440 completed in 0m 6s\n",
      "Loss 0.002538225504566573 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 441 completed in 0m 6s\n",
      "Loss 0.0025376003267524165 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 442 completed in 0m 6s\n",
      "Loss 0.0025380668750206014 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 443 completed in 0m 6s\n",
      "Loss 0.002538260908249404 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 444 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025385338215311018 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 445 completed in 0m 7s\n",
      "Loss 0.0025375645361944204 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 446 completed in 0m 7s\n",
      "Loss 0.0025376894322940474 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 447 completed in 0m 7s\n",
      "Loss 0.0025372648480746544 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 448 completed in 0m 7s\n",
      "Loss 0.0025384683655465144 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 449 completed in 0m 7s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.002538375032796001 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 450 completed in 0m 7s\n",
      "Loss 0.002538533509032402 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 451 completed in 0m 7s\n",
      "Loss 0.002538482284134958 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 452 completed in 0m 7s\n",
      "Loss 0.0025374220514839336 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 453 completed in 0m 7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.002538149843720706 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 454 completed in 0m 7s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.002536032444022137 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 455 completed in 0m 6s\n",
      "Loss 0.002538170559866425 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 456 completed in 0m 6s\n",
      "Loss 0.002536575690437867 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 457 completed in 0m 6s\n",
      "Loss 0.0025383521150484038 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 458 completed in 0m 6s\n",
      "Loss 0.0025380762858960688 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 459 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025385511978897823 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 460 completed in 0m 6s\n",
      "Loss 0.0025380952361154273 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 461 completed in 0m 6s\n",
      "Loss 0.0025382722197911864 is bigger than Loss 0.0025332742707759957 in the prev epoch \n",
      "Loss = 0.0025 at epoch 462 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0025 at epoch 463 completed in 0m 6s\n",
      "Loss 0.0025384624839269614 is bigger than Loss 0.0025327841531577852 in the prev epoch \n",
      "Loss = 0.0025 at epoch 464 completed in 0m 6s\n",
      "Loss 0.002537615860946421 is bigger than Loss 0.0025327841531577852 in the prev epoch \n",
      "Loss = 0.0025 at epoch 465 completed in 0m 6s\n",
      "Loss 0.0025384180373802267 is bigger than Loss 0.0025327841531577852 in the prev epoch \n",
      "Loss = 0.0025 at epoch 466 completed in 0m 6s\n",
      "save the weights\n",
      "Loss = 0.0025 at epoch 467 completed in 0m 6s\n",
      "Loss 0.0025376801137766973 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 468 completed in 0m 6s\n",
      "Loss 0.0025384452123224702 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 469 completed in 0m 6s\n",
      "Loss 0.0025373919138305446 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 470 completed in 0m 6s\n",
      "Loss 0.0025381747693386286 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 471 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.002537945143742054 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 472 completed in 0m 6s\n",
      "Loss 0.0025378567230713643 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 473 completed in 0m 6s\n",
      "Loss 0.0025380269704842855 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 474 completed in 0m 6s\n",
      "Loss 0.0025381042275331605 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 475 completed in 0m 6s\n",
      "Loss 0.0025375750266064135 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 476 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025380587286428736 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 477 completed in 0m 6s\n",
      "Loss 0.00253777472298006 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 478 completed in 0m 6s\n",
      "Loss 0.002537192770723657 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 479 completed in 0m 6s\n",
      "Loss 0.002537820221576367 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 480 completed in 0m 6s\n",
      "Loss 0.002538579429291277 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 481 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.002537113564894049 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 482 completed in 0m 6s\n",
      "Loss 0.0025384945101935654 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 483 completed in 0m 6s\n",
      "Loss 0.0025380889552736314 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 484 completed in 0m 6s\n",
      "Loss 0.0025375500603215246 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 485 completed in 0m 6s\n",
      "Loss 0.0025382082197330695 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 486 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.002537928143626327 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 487 completed in 0m 6s\n",
      "Loss 0.0025381570431460246 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 488 completed in 0m 6s\n",
      "Loss 0.0025383276781200496 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 489 completed in 0m 6s\n",
      "Loss 0.002537373504628858 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 490 completed in 0m 6s\n",
      "Loss 0.002538337437702526 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 491 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025382518910595964 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 492 completed in 0m 6s\n",
      "Loss 0.0025377380528396327 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 493 completed in 0m 6s\n",
      "Loss 0.00253839901954591 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 494 completed in 0m 6s\n",
      "Loss 0.0025373054762380287 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 495 completed in 0m 6s\n",
      "Loss 0.0025379254477982866 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 496 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.002537829653177291 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 497 completed in 0m 6s\n",
      "Loss 0.0025384033164275997 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 498 completed in 0m 6s\n",
      "Loss 0.0025383873462150957 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 499 completed in 0m 6s\n",
      "Loss 0.0025384895073725013 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 500 completed in 0m 6s\n",
      "Loss 0.0025378388400904535 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 501 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025373246061759875 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 502 completed in 0m 6s\n",
      "Loss 0.002538256543458779 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 503 completed in 0m 7s\n",
      "Loss 0.002538144961920621 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 504 completed in 0m 6s\n",
      "Loss 0.002538316720870859 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 505 completed in 0m 6s\n",
      "Loss 0.0025383298799179213 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 506 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.002537667636170843 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 507 completed in 0m 6s\n",
      "Loss 0.002538316440023798 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 508 completed in 0m 6s\n",
      "Loss 0.0025385140089760295 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 509 completed in 0m 6s\n",
      "Loss 0.0025382592005307103 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 510 completed in 0m 6s\n",
      "Loss 0.002537849868678328 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 511 completed in 0m 6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025382753003385487 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 512 completed in 0m 6s\n",
      "Loss 0.002536655592799554 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 513 completed in 0m 6s\n",
      "Loss 0.0025342745017158407 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 514 completed in 0m 6s\n",
      "Loss 0.002537671970877709 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 515 completed in 0m 6s\n",
      "Loss 0.002537284634514672 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 516 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025375756430784584 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 517 completed in 0m 6s\n",
      "Loss 0.002538341494502112 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 518 completed in 0m 6s\n",
      "Loss 0.0025375105756349097 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 519 completed in 0m 6s\n",
      "Loss 0.002538183083822827 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 520 completed in 0m 6s\n",
      "Loss 0.0025375379971267725 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 521 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025380057719205295 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 522 completed in 0m 6s\n",
      "Loss 0.0025374081807159306 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 523 completed in 0m 6s\n",
      "Loss 0.002538463859646409 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 524 completed in 0m 6s\n",
      "Loss 0.002537271041898936 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 525 completed in 0m 6s\n",
      "Loss 0.0025383723816036342 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 526 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025381605467768467 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 527 completed in 0m 6s\n",
      "Loss 0.002538303224826922 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 528 completed in 0m 6s\n",
      "Loss 0.0025382807574537525 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 529 completed in 0m 6s\n",
      "Loss 0.0025380040651817575 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 530 completed in 0m 6s\n",
      "Loss 0.0025350327970207645 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 531 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025381371655238023 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 532 completed in 0m 6s\n",
      "Loss 0.002535592350988847 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 533 completed in 0m 6s\n",
      "Loss 0.00253743241871911 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 534 completed in 0m 6s\n",
      "Loss 0.002538126443849048 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 535 completed in 0m 6s\n",
      "Loss 0.0025379618062239514 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 536 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025382012334418625 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 537 completed in 0m 6s\n",
      "Loss 0.002536563755123587 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 538 completed in 0m 6s\n",
      "Loss 0.0025376250777473554 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 539 completed in 0m 6s\n",
      "Loss 0.0025370799551634863 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 540 completed in 0m 6s\n",
      "Loss 0.0025379657431784716 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 541 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.002538228023566794 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 542 completed in 0m 6s\n",
      "Loss 0.0025380987762975236 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 543 completed in 0m 6s\n",
      "Loss 0.002535967114938089 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 544 completed in 0m 6s\n",
      "Loss 0.002537243976414532 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 545 completed in 0m 6s\n",
      "Loss 0.002538047887906998 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 546 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025374031135136665 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 547 completed in 0m 6s\n",
      "Loss 0.002538539520590267 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 548 completed in 0m 6s\n",
      "Loss 0.0025380083308037833 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 549 completed in 0m 6s\n",
      "Loss 0.0025374872629767495 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 550 completed in 0m 6s\n",
      "Loss 0.002532756392709186 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 551 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025380031915768824 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 552 completed in 0m 6s\n",
      "Loss 0.002538221151143097 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 553 completed in 0m 6s\n",
      "Loss 0.002537865789502511 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 554 completed in 0m 6s\n",
      "Loss 0.0025379801852438897 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 555 completed in 0m 6s\n",
      "Loss 0.002538271345353374 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 556 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.002536986735300552 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 557 completed in 0m 6s\n",
      "Loss 0.002538096116971763 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 558 completed in 0m 6s\n",
      "Loss 0.002534695109881629 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 559 completed in 0m 6s\n",
      "Loss 0.002537940991595575 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 560 completed in 0m 6s\n",
      "Loss 0.0025380287518934905 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 561 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.002538160092482713 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 562 completed in 0m 6s\n",
      "Loss 0.002538056043545033 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 563 completed in 0m 6s\n",
      "Loss 0.0025381378634767734 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 564 completed in 0m 6s\n",
      "Loss 0.002538140557148975 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 565 completed in 0m 6s\n",
      "Loss 0.002538013288058239 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 566 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025369258135359606 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 567 completed in 0m 6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.0025381930417915097 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 568 completed in 0m 6s\n",
      "Loss 0.002537688606264624 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 569 completed in 0m 6s\n",
      "Loss 0.0025376285807902207 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 570 completed in 0m 6s\n",
      "Loss 0.002538372862551781 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 571 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.002536872373611639 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 572 completed in 0m 6s\n",
      "Loss 0.0025378788858832434 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 573 completed in 0m 6s\n",
      "Loss 0.0025382729552753617 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 574 completed in 0m 6s\n",
      "Loss 0.0025384071651886942 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 575 completed in 0m 6s\n",
      "Loss 0.0025381077579159903 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 576 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.002536313900894952 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 577 completed in 0m 6s\n",
      "Loss 0.002536488875279497 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 578 completed in 0m 6s\n",
      "Loss 0.0025381209514564805 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 579 completed in 0m 6s\n",
      "Loss 0.0025383833237619517 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 580 completed in 0m 6s\n",
      "Loss 0.002538255720222152 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 581 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.002537962700897258 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 582 completed in 0m 6s\n",
      "Loss 0.0025382749866639325 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 583 completed in 0m 6s\n",
      "Loss 0.002538291319890372 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 584 completed in 0m 6s\n",
      "Loss 0.0025374624768024316 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 585 completed in 0m 6s\n",
      "Loss 0.0025355299765805438 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 586 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025377819382801942 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 587 completed in 0m 6s\n",
      "Loss 0.002538213339165442 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 588 completed in 0m 6s\n",
      "Loss 0.002538187739161795 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 589 completed in 0m 6s\n",
      "Loss 0.002537952542092545 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 590 completed in 0m 6s\n",
      "Loss 0.0025383855976824406 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 591 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025372775369527597 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 592 completed in 0m 6s\n",
      "Loss 0.002536946830715239 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 593 completed in 0m 6s\n",
      "Loss 0.0025370728739173543 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 594 completed in 0m 6s\n",
      "Loss 0.0025374506072412466 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 595 completed in 0m 6s\n",
      "Loss 0.002538271302138596 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 596 completed in 0m 6s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0025382265877778236 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 597 completed in 0m 6s\n",
      "Loss 0.00253710305027786 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 598 completed in 0m 6s\n",
      "Loss 0.002537638882468284 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 599 completed in 0m 6s\n",
      "Loss 0.0025381328431640124 is bigger than Loss 0.0025311871615504252 in the prev epoch \n",
      "Loss = 0.0025 at epoch 600 completed in 0m 6s\n"
     ]
    }
   ],
   "source": [
    "#MUST UNCOMMENT BELOW LINE...\n",
    "    \n",
    "net = net.cuda()\n",
    "\n",
    "#loading the model after the weights of epoch50.. to check what loss the model gives if lr is taken as 0.0001\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "#criterion = RMSELoss()\n",
    "#criterion = BerhuLoss()\n",
    "#criterion = EuclideanLoss()\n",
    "#criterion = nn.MSELoss()\n",
    "#criterion = CosineLoss()\n",
    "#criterion = torch.nn.MSELoss(size_average=False)\n",
    "criterion = CustomLoss()\n",
    "criterion.cuda()\n",
    "\n",
    "currepochloss = float('Inf')\n",
    "#epochs, n_examples, i, batch_size, flag = 1,5900, 0, 5, 0\n",
    "epochs, n_examples, i, batch_size, flag = 600, 11884, 0, 20, 0\n",
    "\n",
    "\n",
    "while i != epochs:\n",
    "    since = time.time()\n",
    "    cost, batchloss = 0.0, 0.0\n",
    "    num_batches = n_examples//batch_size\n",
    "    #print num_batches    #indices = np.random.permutation(5600)\n",
    "    #indices = np.random.permutation(3524)\n",
    "    \n",
    "    #indices = np.random.permutation(5900)\n",
    "    indices = np.random.permutation(n_examples)\n",
    "    samplesUnprocessed = np.size(indices)\n",
    "    \n",
    "    #batchwise training starts here...\n",
    "    for k in range(num_batches):\n",
    "        since1 = time.time()\n",
    "       # print(\"bacth number:\"+str(k))\n",
    "        xtrain3 = torch.FloatTensor(batch_size,3,180,320)\n",
    "        ytrain3 = torch.FloatTensor(batch_size,2)\n",
    "        ##validPixel = torch.FloatTensor(batch_size,480,640)\n",
    "        \n",
    "        for ind in range(batch_size):\n",
    "            #ind1 = np.random.randint(0,5599)\n",
    "            ind1 = np.random.randint(0,samplesUnprocessed)\n",
    "            #ind1 = np.random.randint(0,794)\n",
    "            #ind1 = np.random.randint(0,794)            \n",
    "            newxind = indices[ind1]            \n",
    "            xtrain3[ind] = xtrainT[newxind]\n",
    "            ytrain3[ind] = ytrainT[newxind]\n",
    "            ##validPixel[ind] = imgValidTrain2[newxind]\n",
    "            \n",
    "            #print ytrain3[ind,0,0,0], ytrain2[newxind,0,0,0]\n",
    "            indices = np.delete(indices,ind1)\n",
    "            samplesUnprocessed = samplesUnprocessed - 1\n",
    "        \n",
    "        #start, end = k*batch_size, (k+1)*batch_size\n",
    "        #batchloss = train(model5,criterion, optimizer, xtrain3, ytrain3, validPixel,batch_size)\n",
    "        batchloss = train(net,criterion, optimizer, xtrain3, ytrain3)\n",
    "        batch_time = time.time() - since1\n",
    "        #cost += batchloss\n",
    "        cost = (cost*k+batchloss)/(k+1)\n",
    "        #print k,cost\n",
    "        #print(\"No. of samples UnProcessed \"+str(samplesUnprocessed))\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    epochloss = cost #/num_batches\n",
    "    \n",
    "    if epochloss < currepochloss:\n",
    "        print('save the weights')\n",
    "        torch.save(net.state_dict(),\"./weights/corridor_new_data_bgr_600.pth\")\n",
    "        flag = 0\n",
    "        currepochloss = epochloss\n",
    "    else:\n",
    "        flag += 1\n",
    "        \n",
    "        if flag == 5:\n",
    "            for p in optimizer.param_groups:\n",
    "                lr2 = p['lr']\n",
    "            newlr = lr2/5\n",
    "            \n",
    "            if newlr < 1e-15:\n",
    "                print(\"Cant decrease further!!\")\n",
    "                newlr = 1e-15\n",
    "            flag = 0 \n",
    "            optimizer = optim.SGD(net.parameters(), lr=newlr, momentum=0.9)\n",
    "            print(\"Learning rate changed from \"+str(lr2)+\" to \"+str(newlr))\n",
    "            \n",
    "        print(\"Loss \"+str(epochloss)+\" is bigger than Loss \"+str(currepochloss)+\" in the prev epoch \")\n",
    "        \n",
    "    print('Loss = {:.4f} at epoch {:d} completed in {:.0f}m {:.0f}s'.format(epochloss,(i+1),(time_elapsed//60),(time_elapsed%60)))\n",
    "    i += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-15\n"
     ]
    }
   ],
   "source": [
    "for params in optimizer.param_groups:\n",
    "    print(params['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(\"./weights/corridor_new_data_bgr_600.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalpred size is --->  torch.Size([2516, 1])\n",
      "num of batches ---> 629\n"
     ]
    }
   ],
   "source": [
    "#testing of the architecture...\n",
    "num_batches = 0\n",
    "#6 evenly divides the test batch size..\n",
    "test_batch_size = 4\n",
    "n_examples = 2516\n",
    "#finalpred = Variable(torch.zeros((n_examples,3,120,160)))\n",
    "finalpred = Variable(torch.zeros((n_examples,1)))\n",
    "print(\"finalpred size is ---> \", finalpred.size())\n",
    "\n",
    "num_batches = n_examples//test_batch_size\n",
    "print(\"num of batches --->\", num_batches)\n",
    "for k in range(num_batches):\n",
    "    start, end = k*test_batch_size, (k+1)*test_batch_size\n",
    "    output = net.forward(Variable(xtestT[start:end], volatile=True).cuda())\n",
    "    finalpred[start:end] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2516, 1)\n"
     ]
    }
   ],
   "source": [
    "data1 = finalpred.data.numpy()\n",
    "print(data1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2516])\n",
      "MSEloss==0.004809698750117668 ABSloss==0.049619811612206535\n"
     ]
    }
   ],
   "source": [
    "dif = torch.abs(finalpred.data[:,0]-ytestT[:,0])\n",
    "print(dif.size())\n",
    "np.savetxt(\"diff.csv\", dif.numpy(), delimiter=\",\")\n",
    "MSEloss = torch.mean(torch.pow(dif,2))\n",
    "ABSloss = torch.mean(dif)\n",
    "print(\"MSEloss==\"+str(MSEloss),\"ABSloss==\"+str(ABSloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7555224895477295\n",
      "1.5707963705062866\n"
     ]
    }
   ],
   "source": [
    "print(finalpred.data[45,0])\n",
    "print(ytestT[45,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8430057856136677"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ABSloss*(180/np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  91.1535\n",
      "  91.1535\n",
      "  91.1535\n",
      "    ⋮    \n",
      " 129.7692\n",
      "  50.2308\n",
      " 129.7692\n",
      "[torch.FloatTensor of size 2516]\n",
      "\n",
      "torch.Size([2516])\n"
     ]
    }
   ],
   "source": [
    "#print(ytestT[:,0]*(180/np.pi))\n",
    "print(ytestT[:,0]*(180/np.pi))\n",
    "a = ytestT[:,0]*(180/np.pi)\n",
    "print(a.size())\n",
    "np.savetxt(\"test.csv\", a.numpy(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.asarray([ [1,2,3], [4,5,6], [7,8,9] ])\n",
    "np.savetxt(\"foo.csv\", a, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  92.9753\n",
      "  83.3433\n",
      "  72.0356\n",
      "    ⋮    \n",
      " 129.8242\n",
      "  46.5208\n",
      " 122.5230\n",
      "[torch.FloatTensor of size 2516]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(finalpred.data[:,0]*(180/np.pi))\n",
    "print(finalpred.data[:,0]*(180/np.pi))\n",
    "b = finalpred.data[:,0]*(180/np.pi)\n",
    "np.savetxt(\"pred.csv\", b.numpy(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4546151161193848"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalpred.data[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual angle===90.34724249169244 1.5768568515777588\n",
      "Angle===95.51048301362856 1.6669723987579346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.291831180523293"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPkAAACiCAYAAACDBmukAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAnYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f97448f98d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['image.interpolation'] = 'none'\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(9)\n",
    "fig.set_figwidth(9)\n",
    "import cv2\n",
    "'''\n",
    "test = cv2.imread(\"./Test_Net_image/63.JPG\")\n",
    "print(test.shape)\n",
    "test = imresize(test,(180,320,3))\n",
    "#imshow(test)\n",
    "test = test.transpose(2,0,1)\n",
    "test = np.reshape(test,(1,3,180,320))\n",
    "test = test.astype(np.float32)\n",
    "testPT = torch.from_numpy(test).float()\n",
    "testPT = batch_rgb_to_bgr(testPT)\n",
    "testPT = torch.div(testPT,255.0)\n",
    "mn = [0.406,0.456,0.485]\n",
    "sd = [0.225,0.224,0.229]\n",
    "norm = Normalize(mn,sd)\n",
    "testPT = norm(testPT)\n",
    "'''\n",
    "ind = 2000\n",
    "testPT = xtestT[ind]\n",
    "print(\"Actual angle===\"+str(ytestT[ind,0]*(180/np.pi)), ytestT[ind,0])\n",
    "testPT = testPT.view(1,3,180,320)\n",
    "#'''\n",
    "test_pred = net.forward(Variable(testPT, volatile=True).cuda())\n",
    "print(\"Angle===\"+str(test_pred.data[0,0]*(180/np.pi)), test_pred.data[0,0])\n",
    "testx = testPT.numpy()\n",
    "testx = np.reshape(testx,(3,180,320))\n",
    "testx = testx.transpose(1,2,0)\n",
    "testx = imresize(testx,(180,320,3))\n",
    "#imshow(testx)\n",
    "scipy.misc.imsave('test.png', testx)\n",
    "a=fig.add_subplot(1,2,1)\n",
    "imgplot = plt.imshow(testx)\n",
    "a.set_title('Input')\n",
    "a.axes.get_xaxis().set_visible(False)\n",
    "a.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
